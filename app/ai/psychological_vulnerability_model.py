import torch
import torch.nn as nn
import torch.nn.functional as F
from transformers import AutoTokenizer, AutoModel, BertTokenizer, BertModel
from torch.utils.data import Dataset, DataLoader
import numpy as np
import pandas as pd
from typing import Dict, List, Tuple, Optional
import json
import pickle
from datetime import datetime
import random
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, confusion_matrix
import warnings
warnings.filterwarnings('ignore')

class VulnerabilityDataset(Dataset):
    def __init__(self, texts, labels, tokenizer, max_length=512):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_length = max_length
    
    def __len__(self):
        return len(self.texts)
    
    def __getitem__(self, idx):
        text = str(self.texts[idx])
        label = self.labels[idx]
        
        encoding = self.tokenizer(
            text,
            truncation=True,
            padding='max_length',
            max_length=self.max_length,
            return_tensors='pt'
        )
        
        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'labels': torch.tensor(label, dtype=torch.float)
        }

class MultiModalVulnerabilityEncoder(nn.Module):
    def __init__(self, hidden_dim=768, num_vulnerability_types=12):
        super().__init__()
        
        self.bert = BertModel.from_pretrained('bert-base-uncased')
        self.text_projection = nn.Linear(768, hidden_dim)
        
        self.behavior_encoder = nn.Sequential(
            nn.Linear(50, 128),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(128, 256),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(256, hidden_dim)
        )
        
        self.social_context_encoder = nn.Sequential(
            nn.Linear(30, 64),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(64, 128),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(128, hidden_dim)
        )
        
        self.temporal_encoder = nn.Sequential(
            nn.Linear(20, 64),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(64, hidden_dim)
        )
        
        self.fusion_layer = nn.MultiheadAttention(
            embed_dim=hidden_dim,
            num_heads=8,
            dropout=0.1,
            batch_first=True
        )
        
        self.vulnerability_classifier = nn.Sequential(
            nn.Linear(hidden_dim * 4, 1024),
            nn.ReLU(),
            nn.Dropout(0.4),
            nn.Linear(1024, 512),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(256, num_vulnerability_types)
        )
        
        self.risk_assessor = nn.Sequential(
            nn.Linear(hidden_dim * 4, 512),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(256, 1),
            nn.Sigmoid()
        )
        
        self.attack_vector_generator = nn.Sequential(
            nn.Linear(hidden_dim * 4, 1024),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(1024, 512),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Linear(256, 20)
        )
        
    def forward(self, input_ids, attention_mask, behavior_features, social_features, temporal_features):
        bert_output = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        text_features = self.text_projection(bert_output.pooler_output)
        
        behavior_encoded = self.behavior_encoder(behavior_features)
        social_encoded = self.social_context_encoder(social_features)
        temporal_encoded = self.temporal_encoder(temporal_features)
        
        combined_features = torch.stack([text_features, behavior_encoded, social_encoded, temporal_encoded], dim=1)
        
        attended_features, _ = self.fusion_layer(combined_features, combined_features, combined_features)
        attended_features = attended_features.view(attended_features.size(0), -1)
        
        vulnerability_scores = self.vulnerability_classifier(attended_features)
        risk_level = self.risk_assessor(attended_features)
        attack_vectors = self.attack_vector_generator(attended_features)
        
        return vulnerability_scores, risk_level, attack_vectors

class PsychologicalVulnerabilityModel:
    def __init__(self, model_path=None):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
        self.model = MultiModalVulnerabilityEncoder()
        self.scaler = StandardScaler()
        
        self.vulnerability_types = [
            'authority_compliance', 'social_proof', 'reciprocity_bias',
            'commitment_consistency', 'liking_similarity', 'scarcity_urgency',
            'curiosity_gap', 'fear_avoidance', 'greed_opportunity',
            'empathy_exploitation', 'ego_validation', 'cognitive_overload'
        ]
        
        self.attack_vectors = [
            'phishing_email', 'social_media_impersonation', 'voice_spoofing',
            'deepfake_video', 'psychological_manipulation', 'authority_abuse',
            'urgency_creation', 'false_reciprocity', 'social_engineering',
            'cognitive_bias_exploit', 'emotional_trigger', 'information_overload',
            'trust_abuse', 'fear_appeal', 'curiosity_hook', 'greed_temptation',
            'empathy_exploit', 'ego_flattery', 'confirmation_bias', 'anchoring_bias'
        ]
        
        if model_path:
            self.load_model(model_path)
    
    def preprocess_text_features(self, text_data):
        features = []
        for text in text_data:
            encoding = self.tokenizer(
                text,
                truncation=True,
                padding='max_length',
                max_length=512,
                return_tensors='pt'
            )
            features.append({
                'input_ids': encoding['input_ids'],
                'attention_mask': encoding['attention_mask']
            })
        return features
    
    def generate_behavior_features(self, person_data):
        features = np.zeros(50)
        
        if 'social_media_activity' in person_data:
            activity = person_data['social_media_activity']
            features[0] = activity.get('posts_per_day', 0) / 10
            features[1] = activity.get('response_rate', 0)
            features[2] = activity.get('friend_count', 0) / 1000
            features[3] = activity.get('followers_ratio', 0)
            features[4] = activity.get('engagement_rate', 0)
        
        if 'communication_patterns' in person_data:
            comm = person_data['communication_patterns']
            features[5] = comm.get('avg_message_length', 0) / 100
            features[6] = comm.get('response_time_hours', 0) / 24
            features[7] = comm.get('emojis_usage_rate', 0)
            features[8] = comm.get('formality_level', 0)
            features[9] = comm.get('question_frequency', 0)
        
        if 'online_behavior' in person_data:
            behavior = person_data['online_behavior']
            features[10] = behavior.get('privacy_settings_level', 0)
            features[11] = behavior.get('public_posts_ratio', 0)
            features[12] = behavior.get('location_sharing', 0)
            features[13] = behavior.get('personal_info_disclosure', 0)
            features[14] = behavior.get('clickbait_susceptibility', 0)
        
        if 'psychological_indicators' in person_data:
            psych = person_data['psychological_indicators']
            features[15] = psych.get('risk_taking_tendency', 0)
            features[16] = psych.get('trust_level', 0)
            features[17] = psych.get('curiosity_level', 0)
            features[18] = psych.get('fear_response', 0)
            features[19] = psych.get('greed_susceptibility', 0)
        
        return torch.tensor(features, dtype=torch.float32)
    
    def generate_social_features(self, person_data):
        features = np.zeros(30)
        
        if 'social_network' in person_data:
            network = person_data['social_network']
            features[0] = network.get('network_size', 0) / 1000
            features[1] = network.get('clustering_coefficient', 0)
            features[2] = network.get('betweenness_centrality', 0)
            features[3] = network.get('influence_score', 0)
            features[4] = network.get('trust_network_density', 0)
        
        if 'social_context' in person_data:
            context = person_data['social_context']
            features[5] = context.get('professional_network_ratio', 0)
            features[6] = context.get('personal_network_ratio', 0)
            features[7] = context.get('cross_platform_activity', 0)
            features[8] = context.get('social_validation_seeking', 0)
            features[9] = context.get('group_affiliation_strength', 0)
        
        return torch.tensor(features, dtype=torch.float32)
    
    def generate_temporal_features(self, person_data):
        features = np.zeros(20)
        
        if 'activity_patterns' in person_data:
            patterns = person_data['activity_patterns']
            features[0] = patterns.get('peak_activity_hour', 0) / 24
            features[1] = patterns.get('weekend_activity_ratio', 0)
            features[2] = patterns.get('activity_consistency', 0)
            features[3] = patterns.get('response_time_variance', 0)
            features[4] = patterns.get('seasonal_activity_changes', 0)
        
        return torch.tensor(features, dtype=torch.float32)
    
    def analyze_vulnerability(self, person_data, text_data=None):
        self.model.eval()
        
        with torch.no_grad():
            behavior_features = self.generate_behavior_features(person_data)
            social_features = self.generate_social_features(person_data)
            temporal_features = self.generate_temporal_features(person_data)
            
            if text_data:
                text_features = self.preprocess_text_features([text_data])
                input_ids = text_features[0]['input_ids'].to(self.device)
                attention_mask = text_features[0]['attention_mask'].to(self.device)
            else:
                dummy_text = " ".join([str(v) for v in person_data.values() if isinstance(v, (str, int, float))])
                text_features = self.preprocess_text_features([dummy_text])
                input_ids = text_features[0]['input_ids'].to(self.device)
                attention_mask = text_features[0]['attention_mask'].to(self.device)
            
            behavior_features = behavior_features.unsqueeze(0).to(self.device)
            social_features = social_features.unsqueeze(0).to(self.device)
            temporal_features = temporal_features.unsqueeze(0).to(self.device)
            
            vulnerability_scores, risk_level, attack_vectors = self.model(
                input_ids, attention_mask, behavior_features, social_features, temporal_features
            )
            
            vulnerability_analysis = {}
            for i, vuln_type in enumerate(self.vulnerability_types):
                vulnerability_analysis[vuln_type] = {
                    'score': float(vulnerability_scores[0][i]),
                    'risk_level': 'High' if vulnerability_scores[0][i] > 0.7 else 'Medium' if vulnerability_scores[0][i] > 0.4 else 'Low'
                }
            
            attack_recommendations = {}
            for i, attack_type in enumerate(self.attack_vectors):
                attack_recommendations[attack_type] = float(attack_vectors[0][i])
            
            return {
                'vulnerability_analysis': vulnerability_analysis,
                'overall_risk_score': float(risk_level[0]),
                'attack_recommendations': attack_recommendations,
                'top_vulnerabilities': sorted(vulnerability_analysis.items(), key=lambda x: x[1]['score'], reverse=True)[:5],
                'recommended_attacks': sorted(attack_recommendations.items(), key=lambda x: x[1], reverse=True)[:5]
            }
    
    def train_model(self, training_data, validation_data, epochs=100, batch_size=16, learning_rate=0.001):
        self.model.to(self.device)
        
        train_dataset = VulnerabilityDataset(
            training_data['texts'], training_data['labels'], self.tokenizer
        )
        val_dataset = VulnerabilityDataset(
            validation_data['texts'], validation_data['labels'], self.tokenizer
        )
        
        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
        
        optimizer = torch.optim.AdamW(self.model.parameters(), lr=learning_rate, weight_decay=0.01)
        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, factor=0.5)
        criterion = nn.BCEWithLogitsLoss()
        
        best_val_loss = float('inf')
        patience_counter = 0
        
        for epoch in range(epochs):
            self.model.train()
            train_loss = 0
            
            for batch in train_loader:
                optimizer.zero_grad()
                
                input_ids = batch['input_ids'].to(self.device)
                attention_mask = batch['attention_mask'].to(self.device)
                labels = batch['labels'].to(self.device)
                
                dummy_behavior = torch.randn(batch_size, 50).to(self.device)
                dummy_social = torch.randn(batch_size, 30).to(self.device)
                dummy_temporal = torch.randn(batch_size, 20).to(self.device)
                
                vulnerability_scores, risk_level, attack_vectors = self.model(
                    input_ids, attention_mask, dummy_behavior, dummy_social, dummy_temporal
                )
                
                loss = criterion(vulnerability_scores, labels)
                loss.backward()
                
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
                optimizer.step()
                
                train_loss += loss.item()
            
            self.model.eval()
            val_loss = 0
            
            with torch.no_grad():
                for batch in val_loader:
                    input_ids = batch['input_ids'].to(self.device)
                    attention_mask = batch['attention_mask'].to(self.device)
                    labels = batch['labels'].to(self.device)
                    
                    dummy_behavior = torch.randn(len(input_ids), 50).to(self.device)
                    dummy_social = torch.randn(len(input_ids), 30).to(self.device)
                    dummy_temporal = torch.randn(len(input_ids), 20).to(self.device)
                    
                    vulnerability_scores, risk_level, attack_vectors = self.model(
                        input_ids, attention_mask, dummy_behavior, dummy_social, dummy_temporal
                    )
                    
                    loss = criterion(vulnerability_scores, labels)
                    val_loss += loss.item()
            
            avg_train_loss = train_loss / len(train_loader)
            avg_val_loss = val_loss / len(val_loader)
            
            scheduler.step(avg_val_loss)
            
            if avg_val_loss < best_val_loss:
                best_val_loss = avg_val_loss
                patience_counter = 0
                self.save_model('best_vulnerability_model.pth')
            else:
                patience_counter += 1
            
            if patience_counter >= 15:
                break
            
            if epoch % 10 == 0:
                print(f'Epoch {epoch}: Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}')
    
    def save_model(self, path):
        torch.save({
            'model_state_dict': self.model.state_dict(),
            'vulnerability_types': self.vulnerability_types,
            'attack_vectors': self.attack_vectors,
            'scaler_state': self.scaler
        }, path)
    
    def load_model(self, path):
        checkpoint = torch.load(path, map_location=self.device)
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.vulnerability_types = checkpoint['vulnerability_types']
        self.attack_vectors = checkpoint['attack_vectors']
        self.scaler = checkpoint['scaler_state']
        self.model.to(self.device)

class VulnerabilityDataGenerator:
    def __init__(self):
        self.vulnerability_patterns = {
            'authority_compliance': [
                "always follows instructions from superiors",
                "respects hierarchy and chain of command",
                "defers to experts and authorities",
                "follows company policies strictly"
            ],
            'social_proof': [
                "influenced by peer behavior",
                "follows popular trends",
                "makes decisions based on others' choices",
                "seeks validation from social groups"
            ],
            'reciprocity_bias': [
                "feels obligated to return favors",
                "accepts gifts and feels indebted",
                "helps others expecting help in return",
                "sensitive to social obligations"
            ],
            'commitment_consistency': [
                "maintains consistent public positions",
                "follows through on commitments",
                "avoids contradicting previous statements",
                "values personal integrity highly"
            ],
            'liking_similarity': [
                "attracted to people with similar interests",
                "trusts those who share common ground",
                "influenced by attractive individuals",
                "responds well to compliments"
            ],
            'scarcity_urgency': [
                "makes quick decisions under pressure",
                "reacts to limited-time offers",
                "fears missing opportunities",
                "acts impulsively when time is limited"
            ]
        }
    
    def generate_synthetic_person_data(self, num_samples=1000):
        persons = []
        
        for _ in range(num_samples):
            person = {
                'id': f"person_{random.randint(10000, 99999)}",
                'social_media_activity': {
                    'posts_per_day': random.uniform(0, 10),
                    'response_rate': random.uniform(0, 1),
                    'friend_count': random.randint(50, 5000),
                    'followers_ratio': random.uniform(0.1, 10),
                    'engagement_rate': random.uniform(0.01, 0.5)
                },
                'communication_patterns': {
                    'avg_message_length': random.randint(10, 200),
                    'response_time_hours': random.uniform(0.1, 48),
                    'emojis_usage_rate': random.uniform(0, 0.3),
                    'formality_level': random.uniform(0, 1),
                    'question_frequency': random.uniform(0, 0.5)
                },
                'online_behavior': {
                    'privacy_settings_level': random.uniform(0, 1),
                    'public_posts_ratio': random.uniform(0, 1),
                    'location_sharing': random.uniform(0, 1),
                    'personal_info_disclosure': random.uniform(0, 1),
                    'clickbait_susceptibility': random.uniform(0, 1)
                },
                'psychological_indicators': {
                    'risk_taking_tendency': random.uniform(0, 1),
                    'trust_level': random.uniform(0, 1),
                    'curiosity_level': random.uniform(0, 1),
                    'fear_response': random.uniform(0, 1),
                    'greed_susceptibility': random.uniform(0, 1)
                },
                'social_network': {
                    'network_size': random.randint(100, 10000),
                    'clustering_coefficient': random.uniform(0, 1),
                    'betweenness_centrality': random.uniform(0, 1),
                    'influence_score': random.uniform(0, 1),
                    'trust_network_density': random.uniform(0, 1)
                },
                'social_context': {
                    'professional_network_ratio': random.uniform(0, 1),
                    'personal_network_ratio': random.uniform(0, 1),
                    'cross_platform_activity': random.uniform(0, 1),
                    'social_validation_seeking': random.uniform(0, 1),
                    'group_affiliation_strength': random.uniform(0, 1)
                },
                'activity_patterns': {
                    'peak_activity_hour': random.randint(0, 23),
                    'weekend_activity_ratio': random.uniform(0, 1),
                    'activity_consistency': random.uniform(0, 1),
                    'response_time_variance': random.uniform(0, 1),
                    'seasonal_activity_changes': random.uniform(0, 1)
                }
            }
            persons.append(person)
        
        return persons
    
    def generate_vulnerability_labels(self, persons):
        labels = []
        
        for person in persons:
            label_vector = np.zeros(12)
            
            psych_indicators = person['psychological_indicators']
            
            if psych_indicators['trust_level'] > 0.7:
                label_vector[0] = 1
            if psych_indicators['greed_susceptibility'] > 0.6:
                label_vector[1] = 1
            if person['communication_patterns']['response_rate'] > 0.8:
                label_vector[2] = 1
            if person['online_behavior']['personal_info_disclosure'] > 0.7:
                label_vector[3] = 1
            if psych_indicators['curiosity_level'] > 0.8:
                label_vector[4] = 1
            if psych_indicators['fear_response'] > 0.7:
                label_vector[5] = 1
            
            labels.append(label_vector)
        
        return labels
    
    def generate_text_descriptions(self, persons):
        texts = []
        
        for person in persons:
            text_parts = []
            
            activity = person['social_media_activity']
            text_parts.append(f"Posts {activity['posts_per_day']:.1f} times per day")
            text_parts.append(f"Has {activity['friend_count']} friends")
            text_parts.append(f"Response rate is {activity['response_rate']:.2f}")
            
            comm = person['communication_patterns']
            text_parts.append(f"Average message length is {comm['avg_message_length']} characters")
            text_parts.append(f"Responds within {comm['response_time_hours']:.1f} hours")
            
            behavior = person['online_behavior']
            text_parts.append(f"Privacy settings level: {behavior['privacy_settings_level']:.2f}")
            text_parts.append(f"Shares location: {behavior['location_sharing']:.2f}")
            
            psych = person['psychological_indicators']
            text_parts.append(f"Trust level: {psych['trust_level']:.2f}")
            text_parts.append(f"Risk taking: {psych['risk_taking_tendency']:.2f}")
            text_parts.append(f"Curiosity level: {psych['curiosity_level']:.2f}")
            
            texts.append(" ".join(text_parts))
        
        return texts
    
    def create_training_dataset(self, num_samples=10000):
        persons = self.generate_synthetic_person_data(num_samples)
        labels = self.generate_vulnerability_labels(persons)
        texts = self.generate_text_descriptions(persons)
        
        return {
            'persons': persons,
            'texts': texts,
            'labels': labels
        }
