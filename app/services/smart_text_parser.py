"""
æ™ºèƒ½æ–‡æœ¬è§£æå™¨ï¼ˆåŸºäºjieba + è§„åˆ™å¼•æ“ï¼‰
å®ç°"è¯†åˆ«åˆ°ä»€ä¹ˆå­—æ®µå°±å½•å…¥ä»€ä¹ˆå­—æ®µ"çš„æ™ºèƒ½è§£æ
æ— éœ€å¤–éƒ¨AI APIä¾èµ–
"""

import re
import jieba
import jieba.posseg as pseg
from typing import Dict, Any, List, Optional, Tuple, Set
from datetime import datetime
import difflib

from app.core.logger import logger

# é…ç½®jieba
jieba.setLogLevel(jieba.logging.INFO)

class SmartTextParser:
    """æ™ºèƒ½æ–‡æœ¬è§£æå™¨"""
    
    def __init__(self):
        self.field_patterns = self._build_field_patterns()
        self.value_extractors = self._build_value_extractors()
        # æ·»åŠ æ›´å¤šå­—æ®µæ¨¡å¼
        self._load_extended_patterns()
        self._load_custom_dict()
    
    def _load_custom_dict(self):
        """åŠ è½½è‡ªå®šä¹‰è¯å…¸"""
        custom_words = [
            # èº«ä»½ä¿¡æ¯
            'èº«ä»½è¯å·', 'èº«ä»½è¯å·ç ', 'è¯ä»¶å·', 'æˆ·ç±åœ°', 'æˆ·ç±åœ°å€', 'çœŸæˆ·ç±åœ°å€',
            'å‡ºç”Ÿåœ°', 'ç±è´¯', 'æˆ·å£æ‰€åœ¨åœ°', 'æ˜Ÿåº§', 'ç”Ÿè‚–', 'å±ç›¸',
            # è”ç³»æ–¹å¼
            'æ‰‹æœºå·', 'è”ç³»ç”µè¯', 'è”ç³»æ–¹å¼', 'é‚®ç®±åœ°å€', 'ç”µå­é‚®ç®±',
            # èŒä¸šä¿¡æ¯
            'å·¥ä½œå•ä½', 'èŒä¸š', 'èŒä½', 'å²—ä½', 'å·¥ä½œ', 'ä»äº‹',
            # åœ°å€ä¿¡æ¯
            'å®¶åº­åœ°å€', 'å±…ä½åœ°', 'ä½å€', 'é€šè®¯åœ°å€', 'å¿«é€’åœ°å€',
            # æ—¶é—´ä¿¡æ¯
            'å‡ºç”Ÿå¹´æœˆ', 'å‡ºç”Ÿæ—¥æœŸ', 'ç”Ÿæ—¥', 'å¹´é¾„',
            # åŒæˆ·äººç›¸å…³
            'åŒæˆ·äºº', 'åŒæˆ·', 'å®¶åº­æˆå‘˜', 'æˆ·ä¸»'
        ]
        
        for word in custom_words:
            jieba.add_word(word)
        
        logger.info(f"å·²åŠ è½½ {len(custom_words)} ä¸ªè‡ªå®šä¹‰è¯æ±‡")
    
    def _load_extended_patterns(self):
        """åŠ è½½æ‰©å±•çš„å­—æ®µè¯†åˆ«æ¨¡å¼"""
        # æ‰©å±•å­—æ®µæ¨¡å¼
        extended_fields = {
            'å¾®åš': ['å¾®åš', 'å¾®åšé“¾æ¥', 'weibo', 'weibo_url'],
            'å¾®ä¿¡': ['å¾®ä¿¡', 'å¾®ä¿¡å·', 'wechat', 'wx'],
            'QQ': ['QQ', 'qq', 'QQå·'],
            'è½¦ç‰Œ': ['è½¦ç‰Œ', 'è½¦ç‰Œå·', 'ç‰Œç…§', 'license_plate'],
            'æ‰‹æœºå½’å±åœ°': ['æ‰‹æœºå½’å±åœ°', 'å½’å±åœ°'],
            'åŒºå·': ['åŒºå·', 'area_code'],
            'åŒºåˆ’ä»£ç ': ['åŒºåˆ’ä»£ç ', 'region_code'],
            'è¿è¥å•†': ['è¿è¥å•†', 'ä¸­å›½è”é€š', 'ä¸­å›½ç§»åŠ¨', 'ä¸­å›½ç”µä¿¡'],
        }
        
        # åˆå¹¶åˆ°ç°æœ‰å­—æ®µæ¨¡å¼
        for field_name, patterns in extended_fields.items():
            if field_name not in self.field_patterns:
                self.field_patterns[field_name] = patterns
            else:
                self.field_patterns[field_name].extend(patterns)
        
        # æ‰©å±•å€¼æå–æ¨¡å¼
        extended_extractors = {
             'URL': r'https?://[\w\.\-/\?=&%#]+|www\.[\w\.\-/\?=&%#]+',
             'è½¦ç‰Œå·': r'[äº¬æ´¥æ²ªæ¸å†€è±«äº‘è¾½é»‘æ¹˜çš–é²æ–°è‹æµ™èµ£é„‚æ¡‚ç”˜æ™‹è’™é™•å‰é—½è´µç²¤é’è—å·å®ç¼ä½¿é¢†][A-Z][A-Z0-9]{4,5}[A-Z0-9æŒ‚å­¦è­¦æ¸¯æ¾³]',
             'QQå·': r'(?:QQ[ï¼š:]?|qq[ï¼š:]?)\s*([1-9]\d{4,10})',  # æ›´ç²¾ç¡®çš„QQå·è¯†åˆ«
             'é‚®ç®±_enhanced': r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b',
         }
        
        self.value_extractors.update(extended_extractors)
    
    def _build_field_patterns(self) -> Dict[str, List[str]]:
        """æ„å»ºå­—æ®µè¯†åˆ«æ¨¡å¼"""
        return {
            'å§“å': ['å§“å', 'name', 'åå­—', 'çœŸå®å§“å', 'æœ¬å'],
            'æ€§åˆ«': ['æ€§åˆ«', 'gender', 'sex'],
            'å¹´é¾„': ['å¹´é¾„', 'age', 'å²'],
            'å‡ºç”Ÿæ—¥æœŸ': ['å‡ºç”Ÿæ—¥æœŸ', 'å‡ºç”Ÿå¹´æœˆ', 'ç”Ÿæ—¥', 'birthday', 'birth_date', 'å‡ºç”Ÿæ—¶é—´'],
            'èº«ä»½è¯å·': ['èº«ä»½è¯å·', 'èº«ä»½è¯å·ç ', 'èº«ä»½è¯', 'id_number', 'è¯ä»¶å·', 'èº«ä»½è¯ä»¶å·'],
            'ç”µè¯': ['ç”µè¯', 'æ‰‹æœº', 'æ‰‹æœºå·', 'è”ç³»ç”µè¯', 'phone', 'mobile', 'tel', 'ç”µè¯å·ç '],
            'é‚®ç®±': ['é‚®ç®±', 'é‚®ä»¶', 'email', 'ç”µå­é‚®ç®±', 'é‚®ç®±åœ°å€'],
            'åœ°å€': ['åœ°å€', 'ä½å€', 'address', 'å±…ä½åœ°', 'å®¶åº­åœ°å€', 'é€šè®¯åœ°å€'],
            'èŒä¸š': ['èŒä¸š', 'å·¥ä½œ', 'èŒä½', 'job', 'occupation', 'å²—ä½', 'ä»äº‹'],
            'å…¬å¸': ['å…¬å¸', 'å·¥ä½œå•ä½', 'company', 'å•ä½', 'ä¼ä¸š'],
            'æˆ·ç±åœ°': ['æˆ·ç±åœ°', 'ç±è´¯', 'å‡ºç”Ÿåœ°', 'æˆ·å£æ‰€åœ¨åœ°'],
            'æˆ·ç±åœ°å€': ['æˆ·ç±åœ°å€', 'çœŸæˆ·ç±åœ°å€', 'æˆ·å£åœ°å€', 'æˆ·ç±è¯¦ç»†åœ°å€'],
            'å­¦å†': ['å­¦å†', 'æ•™è‚²', 'education', 'æ¯•ä¸šäº', 'å°±è¯»äº'],
            'æ˜Ÿåº§': ['æ˜Ÿåº§', 'constellation'],
            'ç”Ÿè‚–': ['ç”Ÿè‚–', 'å±ç›¸', 'zodiac'],
            'æ°‘æ—': ['æ°‘æ—', 'ethnicity', 'æ—'],
            'å©šå§»çŠ¶å†µ': ['å©šå§»çŠ¶å†µ', 'å©šå§»', 'å·²å©š', 'æœªå©š', 'ç¦»å¼‚'],
            'è¡€å‹': ['è¡€å‹', 'blood_type'],
            'èº«é«˜': ['èº«é«˜', 'height', 'é«˜'],
            'ä½“é‡': ['ä½“é‡', 'weight', 'é‡'],
            'å¤‡æ³¨': ['å¤‡æ³¨', 'å…¶ä»–', 'è¯´æ˜', 'note', 'remark', 'è¡¥å……'],
        }
    
    def _build_value_extractors(self) -> Dict[str, str]:
        """æ„å»ºå€¼æå–æ¨¡å¼"""
        return {
            'èº«ä»½è¯å·': r'\b(\d{15}|\d{17}[\dXx]|\d{18})\b',
            'æ‰‹æœºå·': r'\b(1[3-9]\d{9})\b',
            'åº§æœºå·': r'\b(0\d{2,3}[-\s]?\d{7,8})\b',
            'é‚®ç®±': r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b',
            'æ—¥æœŸ': r'(\d{4})[å¹´\-/\.]\s*(\d{1,2})[æœˆ\-/\.]\s*(\d{1,2})[æ—¥]?',
            'å¹´ä»½': r'(\d{4})å¹´',
            'æ€§åˆ«': r'[æ€§åˆ«]?\s*([ç”·å¥³])',
            'ç”Ÿè‚–': r'([é¼ ç‰›è™å…”é¾™è›‡é©¬ç¾ŠçŒ´é¸¡ç‹—çŒª])',
            'æ˜Ÿåº§': r'(ç™½ç¾Šåº§|é‡‘ç‰›åº§|åŒå­åº§|å·¨èŸ¹åº§|ç‹®å­åº§|å¤„å¥³åº§|å¤©ç§¤åº§|å¤©èåº§|å°„æ‰‹åº§|æ‘©ç¾¯åº§|æ°´ç“¶åº§|åŒé±¼åº§)',
            'å†œå†å¹´': r'([ç”²ä¹™ä¸™ä¸æˆŠå·±åºšè¾›å£¬ç™¸][å­ä¸‘å¯…å¯è¾°å·³åˆæœªç”³é…‰æˆŒäº¥]å¹´)',
            'å†œå†æœˆæ—¥': r'((?:æ­£|äºŒ|ä¸‰|å››|äº”|å…­|ä¸ƒ|å…«|ä¹|å|å†¬|è…Š)æœˆ\s*(?:åˆ|å|å»¿|å…)?[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]{1,3})',
        }
    
    def parse_text_intelligent(self, text: str) -> Dict[str, Any]:
        """æ™ºèƒ½è§£ææ–‡æœ¬"""
        logger.info("å¼€å§‹æ™ºèƒ½æ–‡æœ¬è§£æ...")
        
        # 1. é¢„å¤„ç†æ–‡æœ¬
        cleaned_text = self._preprocess_text(text)
        
        # 2. åˆ†ç¦»ä¸»äººç‰©å’ŒåŒæˆ·äººä¿¡æ¯
        main_section, household_section = self._split_main_and_household(cleaned_text)
        
        # 3. æå–ä¸»äººç‰©å­—æ®µ
        main_fields = self._extract_all_fields(main_section)
        
        # 4. æå–åŒæˆ·äººä¿¡æ¯
        household_members = []
        if household_section:
            household_members = self._parse_household_members(household_section)
        
        # 5. å­—æ®µåå¤„ç†å’Œä¼˜åŒ–
        optimized_fields = self._post_process_fields(main_fields)
        
        result = {
            'main_person': optimized_fields,
            'household_members': household_members,
            'raw_text': text,
            'parsing_method': 'smart_jieba',
            'confidence_score': self._calculate_confidence(optimized_fields, household_members)
        }
        
        logger.info(f"è§£æå®Œæˆï¼Œæå–åˆ° {len(optimized_fields)} ä¸ªå­—æ®µï¼Œ{len(household_members)} ä¸ªåŒæˆ·äºº")
        return result
    
    def _preprocess_text(self, text: str) -> str:
        """é¢„å¤„ç†æ–‡æœ¬"""
        # æ ‡å‡†åŒ–å†’å·
        text = text.replace('ï¼š', ':')
        # ç§»é™¤ç‰¹æ®Šç¬¦å·å‰åçš„ç©ºæ ¼ï¼Œä½†ä¿ç•™æ¢è¡Œç¬¦
        text = re.sub(r'[ \t]*([ï¼š:ï¼Œã€‚ï¼›;])[ \t]*', r'\1', text)
        # ç§»é™¤è¡Œå†…å¤šä½™ç©ºæ ¼ï¼Œä½†ä¿ç•™æ¢è¡Œç¬¦
        lines = text.split('\n')
        cleaned_lines = []
        for line in lines:
            # åªå¤„ç†è¡Œå†…ç©ºæ ¼ï¼Œä¸å½±å“æ¢è¡Œç¬¦
            cleaned_line = re.sub(r'[ \t]+', ' ', line.strip())
            if cleaned_line:  # åªä¿ç•™éç©ºè¡Œ
                cleaned_lines.append(cleaned_line)
        return '\n'.join(cleaned_lines)
    
    def _split_main_and_household(self, text: str) -> Tuple[str, str]:
        """åˆ†ç¦»ä¸»äººç‰©å’ŒåŒæˆ·äººä¿¡æ¯"""
        lines = text.split('\n')
        main_lines = []
        household_lines = []
        in_household_section = False
        first_person_processed = False
        
        # åŒæˆ·äººå…³é”®è¯
        household_keywords = ['åŒæˆ·äºº', 'åŒæˆ·', 'åŒä½äºº', 'å®¶åº­æˆå‘˜', 'æˆ·ä¸»', 'å®¶å±']
        
        # åˆ†éš”ç¬¦æ¨¡å¼
        separator_patterns = [
            r'-{3,}',      # ---
            r'={3,}',      # ===
            r'â”{3,}',      # â”â”â”
            r'â”€{3,}',      # â”€â”€â”€
            r'__{3,}',     # ___
        ]
        
        # æ™ºèƒ½åˆ†ç¦»ç­–ç•¥ï¼šåªæœ‰æ˜ç¡®æ ‡è¯†çš„åŒæˆ·äººæ‰åˆ†ç¦»ï¼Œå…¶ä½™éƒ½æ˜¯ä¸»äººç‰©
        in_household_mode = False
        temp_household_lines = []
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
            
            # æ£€æµ‹åŒæˆ·äººå…³é”®è¯ - æ˜ç¡®è¿›å…¥åŒæˆ·äººåŒºåŸŸ
            if any(keyword in line for keyword in household_keywords):
                in_household_mode = True
                temp_household_lines.append(line)
                continue
            
            # æ£€æµ‹åˆ†éš”ç¬¦ - å¯èƒ½æ˜¯åŒæˆ·äººåˆ†éš”ç¬¦
            is_separator = any(re.search(pattern, line) for pattern in separator_patterns)
            if is_separator:
                temp_household_lines.append(line)
                continue
            
            # å¦‚æœåœ¨åŒæˆ·äººæ¨¡å¼ä¸‹
            if in_household_mode:
                # æ£€æŸ¥è¿™è¡Œæ˜¯å¦æ˜¯æ–°çš„ä¸»äººç‰©ä¿¡æ¯ï¼ˆé‡å¤çš„å§“åä¿¡æ¯ï¼‰
                if line.startswith('å§“å:'):
                    # æäº¤ä¹‹å‰æ”¶é›†çš„åŒæˆ·äººä¿¡æ¯åˆ°æ­£å¼åˆ—è¡¨
                    household_lines.extend(temp_household_lines)
                    temp_household_lines = []  # æ¸…ç©ºä¸´æ—¶åˆ—è¡¨
                    
                    # é€€å‡ºåŒæˆ·äººæ¨¡å¼ï¼Œå¼€å§‹å¤„ç†æ–°çš„ä¸»äººç‰©ä¿¡æ¯
                    in_household_mode = False
                    main_lines.append(line)
                else:
                    # ç»§ç»­æ”¶é›†åŒæˆ·äººä¿¡æ¯
                    temp_household_lines.append(line)
            else:
                # ä¸»äººç‰©ä¿¡æ¯
                main_lines.append(line)
        
        # å°†å‰©ä½™çš„ä¸´æ—¶åŒæˆ·äººä¿¡æ¯æ·»åŠ åˆ°æ­£å¼åˆ—è¡¨
        household_lines.extend(temp_household_lines)
        
        return '\n'.join(main_lines), '\n'.join(household_lines)
    
    def _extract_all_fields(self, text: str) -> Dict[str, Any]:
        """æå–æ‰€æœ‰å­—æ®µ"""
        fields = {}
        
        # 1. ä½¿ç”¨jiebaåˆ†è¯å’Œè¯æ€§æ ‡æ³¨
        words = list(pseg.cut(text))
        
        # 2. åŸºäºæ¨¡å¼çš„é”®å€¼å¯¹æå–
        kv_fields = self._extract_key_value_pairs(text)
        self._merge_field_dicts(fields, kv_fields)
        
        # 3. åŸºäºæ­£åˆ™çš„ç‰¹æ®Šå€¼æå–
        regex_fields = self._extract_with_regex(text)
        self._merge_field_dicts(fields, regex_fields)
        
        # 4. åŸºäºä¸Šä¸‹æ–‡çš„æ™ºèƒ½æå–
        context_fields = self._extract_with_context(text, words)
        self._merge_field_dicts(fields, context_fields)
        
        # 5. å»é‡å’Œæ ‡å‡†åŒ–
        normalized_fields = self._normalize_fields(fields)
        
        return normalized_fields
    
    def _merge_field_dicts(self, target: Dict[str, Any], source: Dict[str, Any]):
        """æ™ºèƒ½åˆå¹¶å­—æ®µå­—å…¸"""
        for key, value in source.items():
            if key in target:
                # ä½¿ç”¨å·²æœ‰çš„åˆå¹¶é€»è¾‘
                target[key] = self._merge_values(target[key], value)
            else:
                target[key] = value
    
    def _extract_key_value_pairs(self, text: str) -> Dict[str, Any]:
        """æå–é”®å€¼å¯¹"""
        fields = {}
        
        # å¤šç§é”®å€¼å¯¹æ¨¡å¼
        patterns = [
            r'([^ï¼š:\n\r]{1,20})\s*[:ï¼š]\s*([^ï¼Œ\n\rï¼›;]{1,200})',  # åŸºæœ¬æ¨¡å¼ï¼ˆæ‰©å¤§å€¼é•¿åº¦ï¼‰
            r'([^ï¼š:\n\r]{1,20})\s*[ï¼š:]\s*([^ï¼Œ\n\rï¼›;]{1,200})',  # å˜ä½“æ¨¡å¼
            r'([^ï¼š:\n\r]{1,20})\s*ä¸º\s*([^ï¼Œ\n\rï¼›;]{1,100})',     # "ä¸º"è¿æ¥
            r'([^ï¼š:\n\r]{1,20})\s*æ˜¯\s*([^ï¼Œ\n\rï¼›;]{1,100})',     # "æ˜¯"è¿æ¥
        ]
        
        for pattern in patterns:
            matches = re.findall(pattern, text)
            for raw_key, raw_value in matches:
                key = raw_key.strip()
                value = raw_value.strip()
                
                # è¿‡æ»¤æ— æ•ˆé”®å€¼å¯¹
                if (len(key) >= 2 and len(value) >= 1 and 
                    value not in ['', '-', 'æ— ', 'ç©º', 'æœªçŸ¥', 'null', 'N/A']):
                    
                    # æ ‡å‡†åŒ–å­—æ®µå
                    standard_key = self._standardize_field_name(key)
                    
                    # ğŸ”¥ å¤„ç†å¤šå€¼å­—æ®µï¼ˆå†’å·åˆ†éš”ã€åˆ†å·åˆ†éš”ï¼‰
                    processed_value = self._process_multi_value_field(standard_key, value)
                    fields[standard_key] = processed_value
        
        return fields
    
    def _process_multi_value_field(self, field_name: str, value: str) -> Any:
        """å¤„ç†å¤šå€¼å­—æ®µ"""
        # ç”µè¯å­—æ®µï¼šæ”¯æŒå†’å·åˆ†éš”
        if field_name in ['ç”µè¯', 'æ‰‹æœº', 'æ‰‹æœºå·', 'phone', 'mobile']:
            if ':' in value:
                phones = [p.strip() for p in value.split(':') if p.strip()]
                # éªŒè¯æ¯ä¸ªç”µè¯å·ç 
                valid_phones = []
                for phone in phones:
                    clean_phone = re.sub(r'[\s\-\(\)]', '', phone)
                    if re.match(r'^1[3-9]\d{9}$', clean_phone):
                        valid_phones.append(clean_phone)
                    elif len(clean_phone) >= 7:  # å¯èƒ½æ˜¯åº§æœºæˆ–å…¶ä»–æ ¼å¼
                        valid_phones.append(phone)
                return valid_phones if len(valid_phones) > 1 else (valid_phones[0] if valid_phones else value)
        
        # åœ°å€å­—æ®µï¼šæ”¯æŒåˆ†å·åˆ†éš”
        elif field_name in ['åœ°å€', 'å¿«é€’åœ°å€', 'æ”¶è´§åœ°å€', 'address']:
            if ';' in value:
                addresses = [addr.strip() for addr in value.split(';') if addr.strip()]
                return addresses if len(addresses) > 1 else (addresses[0] if addresses else value)
        
        # URLå­—æ®µï¼šè¯†åˆ«é“¾æ¥
        elif field_name in ['å¾®åšé“¾æ¥', 'é“¾æ¥', 'url', 'link']:
            url_pattern = r'https?://[\w\.\-/\?=&%]+|www\.[\w\.\-/\?=&%]+'
            urls = re.findall(url_pattern, value)
            return urls[0] if urls else value
        
        # å…¶ä»–å­—æ®µä¿æŒåŸå€¼
        return value
    
    def _extract_with_regex(self, text: str) -> Dict[str, Any]:
        """ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–ç‰¹æ®Šå­—æ®µ"""
        fields = {}
        
        for field_type, pattern in self.value_extractors.items():
            matches = re.findall(pattern, text)
            if matches:
                if field_type == 'æ—¥æœŸ':
                    # æ—¥æœŸç‰¹æ®Šå¤„ç†
                    for match in matches:
                        if isinstance(match, tuple):
                            year, month, day = match
                            date_str = f"{year}-{month.zfill(2)}-{day.zfill(2)}"
                            fields['å‡ºç”Ÿæ—¥æœŸ'] = date_str
                            break
                elif field_type in ['èº«ä»½è¯å·', 'æ‰‹æœºå·', 'é‚®ç®±', 'URL', 'è½¦ç‰Œå·', 'QQå·', 'é‚®ç®±_enhanced']:
                    # å¤šå€¼å­—æ®µ
                    if field_type == 'èº«ä»½è¯å·':
                        fields['èº«ä»½è¯å·'] = list(set(matches))  # å»é‡
                    elif field_type == 'æ‰‹æœºå·':
                        fields['ç”µè¯'] = list(set(matches))
                    elif field_type in ['é‚®ç®±', 'é‚®ç®±_enhanced']:
                        fields['é‚®ç®±'] = list(set(matches))
                    elif field_type == 'URL':
                        fields['é“¾æ¥'] = list(set(matches))
                    elif field_type == 'è½¦ç‰Œå·':
                        fields['è½¦ç‰Œ'] = list(set(matches))
                    elif field_type == 'QQå·':
                        fields['QQ'] = list(set(matches))
                else:
                    # å•å€¼å­—æ®µ
                    field_map = {
                        'æ€§åˆ«': 'æ€§åˆ«',
                        'ç”Ÿè‚–': 'ç”Ÿè‚–', 
                        'æ˜Ÿåº§': 'æ˜Ÿåº§',
                        'å†œå†å¹´': 'å†œå†å¹´ä»½',
                        'å†œå†æœˆæ—¥': 'å†œå†æ—¥æœŸ'
                    }
                    if field_type in field_map:
                        fields[field_map[field_type]] = matches[0]
        
        return fields
    
    def _extract_with_context(self, text: str, words: List) -> Dict[str, Any]:
        """åŸºäºä¸Šä¸‹æ–‡çš„æ™ºèƒ½æå–"""
        fields = {}
        
        # å®‰å…¨å¤„ç†åˆ†è¯ç»“æœ
        try:
            # å°†åˆ†è¯ç»“æœè½¬æ¢ä¸ºæ–‡æœ¬çª—å£
            word_list = []
            word_flag_list = []
            
            for item in words:
                if hasattr(item, 'word') and hasattr(item, 'flag'):
                    # jieba.possegè¿”å›çš„å¯¹è±¡
                    word_list.append(item.word)
                    word_flag_list.append((item.word, item.flag))
                elif isinstance(item, tuple) and len(item) == 2:
                    # å…ƒç»„æ ¼å¼
                    word_list.append(item[0])
                    word_flag_list.append(item)
                else:
                    # å­—ç¬¦ä¸²æ ¼å¼
                    word_list.append(str(item))
                    word_flag_list.append((str(item), 'n'))
            
            # æŸ¥æ‰¾å¯èƒ½çš„å­—æ®µæŒ‡ç¤ºè¯
            for i, (word, flag) in enumerate(word_flag_list):
                # å¯»æ‰¾å¯èƒ½çš„å­—æ®µå
                standard_field = self._find_field_in_context(word, i, word_list)
                if standard_field:
                    # æå–è¯¥å­—æ®µçš„å€¼
                    value = self._extract_value_for_field(standard_field, i, word_list, text)
                    if value:
                        fields[standard_field] = value
        
        except Exception as e:
            logger.warning(f"ä¸Šä¸‹æ–‡æå–å¤±è´¥: {e}")
        
        return fields
    
    def _find_field_in_context(self, word: str, position: int, word_list: List[str]) -> Optional[str]:
        """åœ¨ä¸Šä¸‹æ–‡ä¸­æŸ¥æ‰¾å­—æ®µå"""
        # æ£€æŸ¥å½“å‰è¯æ˜¯å¦æ˜¯å­—æ®µå
        for field_name, patterns in self.field_patterns.items():
            if word in patterns:
                return field_name
            
            # æ¨¡ç³ŠåŒ¹é…
            for pattern in patterns:
                if difflib.SequenceMatcher(None, word, pattern).ratio() > 0.8:
                    return field_name
        
        # æ£€æŸ¥ç»„åˆè¯ï¼ˆå½“å‰è¯+ä¸‹ä¸€ä¸ªè¯ï¼‰
        if position < len(word_list) - 1:
            combined = word + word_list[position + 1]
            for field_name, patterns in self.field_patterns.items():
                if combined in patterns:
                    return field_name
        
        return None
    
    def _extract_value_for_field(self, field_name: str, position: int, word_list: List[str], full_text: str) -> Optional[str]:
        """ä¸ºç‰¹å®šå­—æ®µæå–å€¼"""
        # åœ¨è¯ä½ç½®é™„è¿‘æŸ¥æ‰¾å€¼
        context_window = 3  # å‰å3ä¸ªè¯çš„çª—å£
        
        start = max(0, position - context_window)
        end = min(len(word_list), position + context_window + 1)
        
        context_words = word_list[start:end]
        context_text = ' '.join(context_words)
        
        # æ ¹æ®å­—æ®µç±»å‹ä½¿ç”¨ä¸åŒç­–ç•¥
        if field_name in ['èº«ä»½è¯å·', 'ç”µè¯', 'é‚®ç®±']:
            # ä½¿ç”¨æ­£åˆ™æå–
            if field_name == 'èº«ä»½è¯å·':
                matches = re.findall(self.value_extractors['èº«ä»½è¯å·'], context_text)
            elif field_name == 'ç”µè¯':
                matches = re.findall(self.value_extractors['æ‰‹æœºå·'], context_text)
            elif field_name == 'é‚®ç®±':
                matches = re.findall(self.value_extractors['é‚®ç®±'], context_text)
            
            return matches[0] if matches else None
        
        # å¯¹äºå…¶ä»–å­—æ®µï¼ŒæŸ¥æ‰¾å†’å·åçš„å†…å®¹
        colon_pattern = rf'{re.escape(field_name)}\s*[:ï¼š]\s*([^ï¼Œ\n\rï¼›;]+)'
        match = re.search(colon_pattern, full_text)
        if match:
            return match.group(1).strip()
        
        return None
    
    def _standardize_field_name(self, raw_field: str) -> str:
        """æ ‡å‡†åŒ–å­—æ®µå"""
        raw_field = raw_field.strip()
        
        # ç²¾ç¡®åŒ¹é…
        for standard_name, patterns in self.field_patterns.items():
            if raw_field in patterns:
                return standard_name
        
        # æ¨¡ç³ŠåŒ¹é…
        best_match = None
        best_score = 0
        
        for standard_name, patterns in self.field_patterns.items():
            for pattern in patterns:
                score = difflib.SequenceMatcher(None, raw_field.lower(), pattern.lower()).ratio()
                if score > best_score and score > 0.8:
                    best_score = score
                    best_match = standard_name
        
        return best_match if best_match else raw_field
    
    def _normalize_fields(self, fields: Dict[str, Any]) -> Dict[str, Any]:
        """å­—æ®µæ ‡å‡†åŒ–å’Œå»é‡"""
        normalized = {}
        
        for key, value in fields.items():
            # åˆå¹¶ç›¸ä¼¼å­—æ®µ
            merged_key = self._find_similar_key(key, list(normalized.keys()))
            final_key = merged_key if merged_key else key
            
            # å€¼å¤„ç†
            if final_key in normalized:
                # åˆå¹¶å€¼
                existing = normalized[final_key]
                merged_value = self._merge_values(existing, value)
                normalized[final_key] = merged_value
            else:
                normalized[final_key] = value
        
        return normalized
    
    def _find_similar_key(self, key: str, existing_keys: List[str], threshold: float = 0.85) -> Optional[str]:
        """æŸ¥æ‰¾ç›¸ä¼¼çš„å·²å­˜åœ¨é”®"""
        for existing_key in existing_keys:
            similarity = difflib.SequenceMatcher(None, key.lower(), existing_key.lower()).ratio()
            if similarity > threshold:
                return existing_key
        return None
    
    def _merge_values(self, existing: Any, new: Any) -> Any:
        """åˆå¹¶å­—æ®µå€¼"""
        if existing == new:
            return existing
        
        # ç‰¹æ®Šå¤„ç†ç”µè¯å­—æ®µçš„å†’å·åˆ†éš”å€¼
        if isinstance(new, str) and ':' in new:
            new_phones = [p.strip() for p in new.split(':') if p.strip() and len(p.strip()) >= 7]
            if new_phones:
                new = new_phones
        
        # è½¬æ¢ä¸ºåˆ—è¡¨è¿›è¡Œåˆå¹¶
        if not isinstance(existing, list):
            existing = [existing] if existing else []
        if not isinstance(new, list):
            new = [new] if new else []
        
        # åˆå¹¶å¹¶å»é‡
        merged = existing.copy()
        for item in new:
            if item and str(item).strip() and item not in merged:
                merged.append(item)
        
        return merged[0] if len(merged) == 1 else merged
    
    def _parse_household_members(self, household_text: str) -> List[Dict[str, Any]]:
        """è§£æåŒæˆ·äººä¿¡æ¯"""
        members = []
        
        # æŒ‰åˆ†éš”ç¬¦åˆ†å‰²æˆå‘˜
        sections = re.split(r'-{3,}|={3,}|\u2501{3,}|\u2014{3,}', household_text)
        
        for section in sections:
            section = section.strip()
            if not section:
                continue
                
            # æ£€æŸ¥æ˜¯å¦åŒ…å«åŒæˆ·äººå…³é”®è¯ä½†ä»æœ‰å®é™…å†…å®¹
            has_household_keyword = any(keyword in section for keyword in ['åŒæˆ·äºº', 'åŒæˆ·', 'å®¶åº­æˆå‘˜'])
            if has_household_keyword:
                # å¤„ç†åŒ…å«åŒæˆ·äººå…³é”®è¯çš„è¡Œï¼Œæå–å®é™…ä¿¡æ¯
                lines = section.split('\n')
                content_lines = []
                for line in lines:
                    line = line.strip()
                    if not line:
                        continue
                    
                    # æ£€æŸ¥æ˜¯å¦æ˜¯ "åŒæˆ·äºº:å§“å,æ€§åˆ«,èº«ä»½è¯,æ—¥æœŸ" æ ¼å¼
                    if any(keyword in line for keyword in ['åŒæˆ·äºº', 'åŒæˆ·', 'å®¶åº­æˆå‘˜']):
                        # æå–å†’å·åçš„å†…å®¹
                        colon_pos = line.find(':')
                        if colon_pos != -1:
                            member_info = line[colon_pos + 1:].strip()
                            if member_info:
                                content_lines.append(member_info)
                    else:
                        content_lines.append(line)
                
                if content_lines:
                    section = '\n'.join(content_lines)
                else:
                    continue
            
            # ä¸ºæ¯ä¸ªæˆå‘˜æå–å­—æ®µ
            member_fields = self._extract_all_fields(section)
            
            # å¦‚æœæ²¡æœ‰æå–åˆ°å§“åï¼Œå°è¯•è§£æé€—å·åˆ†éš”æ ¼å¼
            if not member_fields.get('å§“å'):
                member_fields = self._parse_comma_separated_member(section, member_fields)
            
            # éªŒè¯æ˜¯å¦ä¸ºæœ‰æ•ˆæˆå‘˜ï¼ˆè‡³å°‘æœ‰å§“åæˆ–èº«ä»½è¯ï¼‰
            if (member_fields.get('å§“å') or 
                member_fields.get('èº«ä»½è¯å·') or
                any('å§“å' in str(k) for k in member_fields.keys())):
                members.append(member_fields)
        
        return members
    
    def _parse_comma_separated_member(self, section: str, existing_fields: Dict[str, Any]) -> Dict[str, Any]:
        """è§£æé€—å·åˆ†éš”çš„æˆå‘˜ä¿¡æ¯æ ¼å¼ï¼Œå¦‚ï¼šé‚“é’æ¾,ç”·,512922197305011634,1973å¹´05æœˆ01æ—¥"""
        member_fields = existing_fields.copy()
        
        # æŸ¥æ‰¾å¯èƒ½çš„é€—å·åˆ†éš”è¡Œ
        for line in section.split('\n'):
            line = line.strip()
            if not line or ':' in line:  # è·³è¿‡é”®å€¼å¯¹æ ¼å¼çš„è¡Œ
                continue
                
            parts = [p.strip() for p in line.split(',')]
            if len(parts) >= 3:  # è‡³å°‘æœ‰å§“åã€æ€§åˆ«ã€èº«ä»½è¯
                # å°è¯•è¯†åˆ«å„ä¸ªå­—æ®µ
                potential_name = parts[0]
                potential_gender = parts[1] if len(parts) > 1 else None
                potential_id = parts[2] if len(parts) > 2 else None
                potential_birth = parts[3] if len(parts) > 3 else None
                
                # éªŒè¯èº«ä»½è¯æ ¼å¼æ¥ç¡®å®šè¿™æ˜¯æœ‰æ•ˆçš„æˆå‘˜ä¿¡æ¯
                if potential_id and re.match(r'^[0-9]{15}([0-9]{2}[0-9Xx])?$', potential_id):
                    # æå–å§“åï¼ˆå¦‚æœè¿˜æ²¡æœ‰ï¼‰
                    if not member_fields.get('å§“å') and potential_name and len(potential_name) <= 10:
                        member_fields['å§“å'] = potential_name
                    
                    # æå–æ€§åˆ«ï¼ˆå¦‚æœè¿˜æ²¡æœ‰ï¼‰
                    if not member_fields.get('æ€§åˆ«') and potential_gender in ['ç”·', 'å¥³']:
                        member_fields['æ€§åˆ«'] = potential_gender
                    
                    # èº«ä»½è¯å·å·²ç»é€šè¿‡å¸¸è§„æ–¹æ³•æå–äº†ï¼Œä¸éœ€è¦é‡å¤
                    
                    # å‡ºç”Ÿæ—¥æœŸå·²ç»é€šè¿‡å¸¸è§„æ–¹æ³•æå–äº†ï¼Œä¸éœ€è¦é‡å¤
                    break
        
        return member_fields
    
    def _post_process_fields(self, fields: Dict[str, Any]) -> Dict[str, Any]:
        """å­—æ®µåå¤„ç†"""
        processed = fields.copy()
        
        # æ•°æ®ç±»å‹è½¬æ¢å’ŒéªŒè¯
        if 'å‡ºç”Ÿæ—¥æœŸ' in processed:
            processed['å‡ºç”Ÿæ—¥æœŸ'] = self._normalize_date(processed['å‡ºç”Ÿæ—¥æœŸ'])
        
        if 'ç”µè¯' in processed:
            processed['ç”µè¯'] = self._normalize_phones(processed['ç”µè¯'])
        
        if 'èº«ä»½è¯å·' in processed:
            processed['èº«ä»½è¯å·'] = self._normalize_id_numbers(processed['èº«ä»½è¯å·'])
        
        # ç§»é™¤ç©ºå€¼å’Œæ— æ•ˆå€¼ï¼ˆä¿ç•™éç©ºåˆ—è¡¨ï¼‰
        def is_valid_value(value):
            if isinstance(value, list):
                return len(value) > 0 and any(str(item).strip() for item in value)
            return (value is not None and 
                   str(value).strip() not in ['', '-', 'æ— ', 'ç©º', 'æœªçŸ¥', 'null', 'N/A'])
        
        processed = {k: v for k, v in processed.items() if is_valid_value(v)}
        
        return processed
    
    def _normalize_date(self, date_value: Any) -> str:
        """æ ‡å‡†åŒ–æ—¥æœŸ"""
        if isinstance(date_value, list):
            date_value = date_value[0]
        
        date_str = str(date_value).strip()
        
        # å°è¯•å¤šç§æ—¥æœŸæ ¼å¼
        patterns = [
            r'(\d{4})[å¹´\-/\.]\s*(\d{1,2})[æœˆ\-/\.]\s*(\d{1,2})[æ—¥]?',
            r'(\d{4})\s*(\d{1,2})\s*(\d{1,2})',
        ]
        
        for pattern in patterns:
            match = re.search(pattern, date_str)
            if match:
                year, month, day = match.groups()
                try:
                    # éªŒè¯æ—¥æœŸ
                    datetime(int(year), int(month), int(day))
                    return f"{year}-{month.zfill(2)}-{day.zfill(2)}"
                except ValueError:
                    continue
        
        return date_str
    
    def _normalize_phones(self, phone_value: Any) -> List[str]:
        """æ ‡å‡†åŒ–ç”µè¯å·ç """
        if not isinstance(phone_value, list):
            phone_value = [phone_value]
        
        normalized = []
        for phone in phone_value:
            phone_str = str(phone).strip()
            # æ¸…ç†æ ¼å¼
            clean_phone = re.sub(r'[\s\-\(\)]', '', phone_str)
            # éªŒè¯æ‰‹æœºå·æ ¼å¼
            if re.match(r'^1[3-9]\d{9}$', clean_phone):
                normalized.append(clean_phone)
            else:
                normalized.append(phone_str)  # ä¿ç•™åŸæ ¼å¼
        
        return list(set(normalized))  # å»é‡
    
    def _normalize_id_numbers(self, id_value: Any) -> List[str]:
        """æ ‡å‡†åŒ–èº«ä»½è¯å·"""
        if not isinstance(id_value, list):
            id_value = [id_value]
        
        normalized = []
        for id_num in id_value:
            id_str = str(id_num).strip().upper()
            # åŸºæœ¬æ ¼å¼éªŒè¯
            if re.match(r'^\d{15}$|^\d{17}[\dX]$|^\d{18}$', id_str):
                normalized.append(id_str)
        
        return list(set(normalized))  # å»é‡
    
    def _calculate_confidence(self, main_fields: Dict[str, Any], household_members: List[Dict[str, Any]]) -> float:
        """è®¡ç®—è§£æç½®ä¿¡åº¦"""
        score = 0.5  # åŸºç¡€åˆ†æ•°
        
        # ä¸»è¦å­—æ®µåŠ åˆ†
        key_fields = ['å§“å', 'æ€§åˆ«', 'å‡ºç”Ÿæ—¥æœŸ', 'èº«ä»½è¯å·', 'ç”µè¯']
        for field in key_fields:
            if field in main_fields:
                score += 0.1
        
        # å­—æ®µæ•°é‡åŠ åˆ†
        score += min(len(main_fields) * 0.02, 0.2)
        
        # åŒæˆ·äººä¿¡æ¯åŠ åˆ†
        score += min(len(household_members) * 0.02, 0.1)
        
        # æ•°æ®è´¨é‡åŠ åˆ†
        if 'èº«ä»½è¯å·' in main_fields:
            id_nums = main_fields['èº«ä»½è¯å·']
            if isinstance(id_nums, list) and any(re.match(r'^\d{17}[\dX]$', str(num)) for num in id_nums):
                score += 0.1
        
        return min(score, 1.0)


# åˆ›å»ºå…¨å±€è§£æå™¨å®ä¾‹
smart_parser = SmartTextParser() 